{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg.SOLVER.OPTIMIZER = \"SGD\"\n",
    "cfg.SOLVER.OPTIMIZER = \"Adam\"\n",
    "cfg.SOLVER.OPTIMIZER = \"RMSprop\"\n",
    "cfg.SOLVER.OPTIMIZER = \"Adadelta\"\n",
    "cfg.SOLVER.OPTIMIZER = \"Adagrad\"\n",
    "cfg.SOLVER.OPTIMIZER = \"AdamW\"\n",
    "cfg.SOLVER.OPTIMIZER = \"SparseAdam\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SGD (Stochastic Gradient Descent):Her adımda rastgele bir örneklem seçerek modeli eğitmek için kullanılan en temel optimizasyon algoritmasıdır. Bu algoritma, gradientlerin hesaplanması ve güncellenmesi için bir zaman maliyeti nedeniyle büyük veri setleri üzerinde yavaş çalışabilir. Ancak, küçük veri setleri veya mini toplu eğitim (mini-batch training) durumunda, diğer algoritmalardan daha hızlı çalışabilir. Ayrıca, diğer algoritmalardan daha az hiperparametre gerektirir.\n",
    "\n",
    "# Adam (Adaptive Moment Estimation):Her parametre için ayrı ayrı adaptif öğrenme oranları hesaplayarak öğrenme hızını otomatik olarak ayarlayan bir optimizasyon algoritmasıdır. Bu nedenle, SGD'ye göre daha hızlı yakınsar ve daha geniş bir hiperparametre yelpazesine sahiptir. Ancak, bazı durumlarda aşırı öğrenmeye neden olabilir.\n",
    "\n",
    "# RMSprop (Root Mean Square Propagation): Her adımda tüm gradientlerin karesinin üssünü alarak adaptif öğrenme hızlarını ayarlar. Bu algoritma, Adam'a kıyasla daha az hiperparametre gerektirir ve bazı durumlarda daha iyi sonuçlar verebilir. Ancak, küçük veri setleri üzerinde hızlı yakınsama sağlamayabilir.\n",
    "\n",
    "# Adadelta: Her parametre için adaptif öğrenme hızları hesaplar ve ayrıca gradyanların varyansını da izleyerek öğrenme oranını otomatik olarak ayarlar. Bu algoritma, bazı durumlarda hızlı yakınsama sağlayabilir ve diğer algoritmalara kıyasla daha az hiperparametre gerektirir.\n",
    "\n",
    "# Adagrad: Her parametre için öğrenme hızını ayrı ayrı ayarlayarak günceller ve daha önceki gradyanların karesi ile orantılıdır. Bu algoritma, seyrek veri setleri üzerinde iyi çalışır ve bazı durumlarda hızlı yakınsama sağlayabilir. Ancak, küçük öğrenme oranları için verimli olmayabilir.\n",
    "\n",
    "# AdamW: Adam algoritmasının bir değişikliğidir ve aşırı uydurmaya (overfitting) karşı daha dirençlidir. Bu nedenle, bazı durumlarda daha iyi sonuçlar verebilir.\n",
    "\n",
    "# SparseAdam: Nadir gradyanlar ile çalışmak için optimize edilmiştir. Bu algoritma, nadir gradyanlar içeren büyük veri kümeleri üzerinde daha hızlı çalışır ve diğer algoritmalardan daha az bellek gerektirir. Ancak, seyrek olmayan veri kümeleri üzerinde performansı daha düşük olabilir."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
